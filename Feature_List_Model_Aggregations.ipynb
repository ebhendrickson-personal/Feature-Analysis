{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature List Aggregation\n",
    "This code will pull in the future lists for all directories and combine into a single list. This will be done for all different models that we have to get a better idea what about features are used in what model. This will include aggregating at the Model Level (All submodels for different marketing segments or portions) as well as a full detail leve. \n",
    "\n",
    "## Version:\n",
    "0.1 - Created initial script based on previous Feature List Semi Gross Model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix}  \"pandas >=1.1.0\"  \"s3fs>=0.4.2\" regex boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import boto3 \n",
    "import s3fs\n",
    "\n",
    "from s3fs.core import S3FileSystem\n",
    "#Import new packages\n",
    "import os\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this kernal option to enable me to see the value of maultiple statements at once\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab all files and make a dictionary\n",
    "Create a dictionary that shows the location for each type of model. This will give the easy to understand name for the overall model. This will include seperating propensity vs whatever sales category is used. This will give a comprehensive list of all models that are on-going. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a S3 and client which will be used throughout\n",
    "s3 = boto3.resource('s3')\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "#Create a generic Paginator\n",
    "paginator = s3client.get_paginator('list_objects_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Model Name and Folder Location:\n",
    "model_location_dict = {\n",
    "    \"Yearly_Response\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/yearly_models/propensity_models/\",\n",
    "    \"Yearly_Semi_Gross_Profit\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/yearly_models/semi_gross_profit_models/\",\n",
    "    \"Yearly_Net_Sales_Models\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/yearly_models/net_sales_models/\",\n",
    "    \"Monthly_Response_Models\":  \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/monthly_models/propensity_models/\",\n",
    "    \"Monthly_Net_Sales_Models\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/monthly_models/net_sales_models/\",\n",
    "    \"Marketing_Discount_Amount\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/discount_models/marketing_disc_amt/\",\n",
    "    \"Marketing_Discount_Users\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/discount_models/marketing_disc_user/\",\n",
    "    \"Other_Discount_Amount\":  \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/discount_models/other_disc_amt/\",\n",
    "    \"Other_Discount_Users\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/discount_models/other_disc_user/\",\n",
    "    \"Catalog_Response\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/catalog_models/propensity_models/\", \n",
    "    \"Catalog_Net_Demand_amt\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/catalog_models/net_demand_amt/\",\n",
    "    \"Catalog_Operating_Profit\": \"s3://bluestembrands-bi-obmarketing-zone-prod/Production_Models/catalog_models/operating_profit/\"\n",
    "}\n",
    "\n",
    "model_location_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "There will be a function that gets the list of features for all submodels and one that combines those into a master feature list for each type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function that will create a list of all feature files from an output location:\n",
    "def get_feature_lists_locations(folder_location):\n",
    "    #Find S3:// for any digit character\n",
    "    bucket_search =  re.search(\"s3://[\\\\w-_]*\", folder_location)\n",
    "    #remove the S3 to finish the bucket\n",
    "    bucket_name = re.sub(\"s3://\" ,\"\", bucket_search[0]) \n",
    "    #Define the prekey from the folder\n",
    "    pre_key = re.sub( f'{bucket_name}/', \"\",re.sub(\"s3://\" ,\"\", folder_location))\n",
    "    \n",
    "    #Define response iterator which can be used to search through folders\n",
    "    response_iterator = paginator.paginate(Bucket=bucket_name, Prefix=pre_key)\n",
    "    #Define empty list\n",
    "    feature_name_list = []\n",
    "    \n",
    "    #Search through paths for all different methods to determine feature\n",
    "    for response in response_iterator:\n",
    "        for object_data in response['Contents']:\n",
    "            key = object_data['Key']\n",
    "            if key.endswith('features.csv'):\n",
    "                feature_name_list.append(key)\n",
    "                \n",
    "    return feature_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features( location_list , folder_location ):   \n",
    "    #Find S3:// for any digit character\n",
    "    bucket_search =  re.search(\"s3://[\\\\w-_]*\", folder_location)\n",
    "    #remove the S3 to finish the bucket\n",
    "    bucket_name = re.sub(\"s3://\" ,\"\", bucket_search[0]) \n",
    "    \n",
    "    #temporaty dictionary that will be reduced within this funciton\n",
    "    temp_dict = dict()\n",
    "    \n",
    "    for i in range(len(location_list)):\n",
    "    #Grab the divsion from within the folder structure as only capital letter\n",
    "        name = i\n",
    "        #Define the name\n",
    "        #Load the file if .csv\n",
    "        temp_dict[name] = pd.read_csv(f\"s3://{bucket_name}/{location_list[i]}\")\n",
    "        #If .txt file load as a pickle file\n",
    "    \n",
    "    #Create basic dataframe to allow for joining based on dictionary names\n",
    "    total_features_df = pd.DataFrame( columns = [\"Column_Names\"])\n",
    "    \n",
    "    for k,v in temp_dict.items():\n",
    "        df =  v\n",
    "        df.rename(columns={df.columns[0]: 'Column_Names'}, inplace = True)\n",
    "        #Define the contains column using the key name to create a unqiue list\n",
    "        df[f'{k}_contains'] = True\n",
    "        total_features_df = total_features_df.merge(df, how = 'outer', on = \"Column_Names\" )\n",
    "    \n",
    "    return total_features_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the Features for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model list from model filter regex\n",
    "model_list = list(model_location_dict.keys())\n",
    "\n",
    "# model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an entire list of features with an dictionary to be able to name everything\n",
    "all_model_features_dict = dict()\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    #Define the name\n",
    "    name = model_list[i]\n",
    "    #Grab the feature list\n",
    "    temp_location_list = get_feature_lists_locations( folder_location= model_location_dict[name] )\n",
    "    #Using this list get a list of all features fro this\n",
    "    temp_features_df = get_features(  location_list = temp_location_list, folder_location = model_location_dict[name]   )\n",
    "    #Just get the features by dropping where they are used in teh submodels\n",
    "    feature_array = temp_features_df['Column_Names']\n",
    "    all_model_features_dict[name]  = feature_array\n",
    "    print(f\"Finished with {name}: {i+1} of {len(model_list)}\")\n",
    "    \n",
    "all_model_features_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a blank dataframe which we use to join all model feature files\n",
    "total_features_df = pd.DataFrame(columns = [\"Column_Names\"])\n",
    "\n",
    "#Join all files together to get a dataframe that shows were all features are used\n",
    "for k,v in all_model_features_dict.items():\n",
    "    df =  pd.DataFrame(v, columns = [\"Column_Names\"]  )\n",
    "    #Define the contains column using the key name to create a unqiue list\n",
    "    df[f'{k}_contains'] = True\n",
    "    total_features_df = total_features_df.merge(df, how = 'outer' , on = \"Column_Names\" )\n",
    "    \n",
    "total_features_df.fillna(False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a column that determines how many models it is used for and sort using it\n",
    "total_features_df['Total_Model_Uses'] = total_features_df.sum( axis = 1)\n",
    "total_features_df.sort_values( by = \"Total_Model_Uses\", ascending = False, inplace = True )\n",
    "\n",
    "#View this information\n",
    "total_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in metadata\n",
    "meta_data = pd.read_csv(\"s3://bluestembrands-bi-obmarketing-zone-prod/Feature_Classification/all_features_metadata.csv\")\n",
    "\n",
    "#Rename and drop some columns\n",
    "meta_data.rename( columns = { \"col_name\": \"Column_Names\" }, inplace = True )\n",
    "meta_data.drop( columns  = ['replacement_type'], inplace = True )\n",
    "\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the metadata with the feature data to show all the feature's with their metadata\n",
    "all_features_meta = meta_data.merge(total_features_df, how = 'right' , on = \"Column_Names\" )\n",
    "\n",
    "all_features_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output the features as a .csv file\n",
    "# all_features_meta.to_csv( \"s3://bluestembrands-bi-obmarketing-zone-prod/Feature_Classification/all_model_feature_analysis.csv\", index = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
